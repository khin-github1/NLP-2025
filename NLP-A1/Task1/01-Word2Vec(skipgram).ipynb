{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')  # For tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters as rt \n",
    "rt.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "categories = rt.categories() \n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences: 8458\n",
      "['china', 'daily', 'says', 'vermin', 'eat', '7-12', 'pct', 'grain', 'stocks', 'a', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'china', \"'s\", 'grain', 'stocks', ',', 'the', 'china', 'daily', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "#. tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Get raw text from the reuters corpus\n",
    "raw_text = rt.raw(categories=['fuel', 'gas','gold', 'grain', 'heat', 'housing', 'income', 'interest']) \n",
    "\n",
    "# Tokenize into sentences and then words\n",
    "sentences = sent_tokenize(raw_text)  # Tokenize the raw text into sentences\n",
    "corpus = [word_tokenize(sent.lower()) for sent in sentences]  # Tokenize sentences into words\n",
    "\n",
    "# Check the number of tokenized sentences\n",
    "print(f\"Number of tokenized sentences: {len(corpus)}\")\n",
    "\n",
    "\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8458"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralizaition\n",
    "# find uniquee words\n",
    "flatten=lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs=list(set(flatten(corpus))) # all the words we have in the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hand mapping between iteger and word\n",
    "word2index={v:idx for idx, v in enumerate(vocabs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_idx=len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14269"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK for unknown\n",
    "vocabs.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>']= lst_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word={v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of center word, and outsidee word\n",
    "def random_batch(batch_size,corpus):\n",
    "    skipgrams=[]\n",
    "    # loop  each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 3rd word until third last word since window size =2\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center=word2index[doc[i]]\n",
    "            #outside words=2 words\n",
    "            outside = (word2index[doc[i-2]],word2index[doc[i-1]], \n",
    "                       word2index[doc[i+1]],word2index[doc[i+2]])\n",
    "            #print(center, outside)\n",
    "            #for each for these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                #print(each_out)\n",
    "                skipgrams.append([center,each_out])\n",
    "            # center, outeside1; center, outside2\n",
    "\n",
    "    random_index=np.random.choice(range(len(skipgrams)),batch_size,replace=False)\n",
    "    input, label=[], []\n",
    "    for index in random_index:\n",
    "        input.append([skipgrams[index][0]])\n",
    "        label.append([skipgrams[index][1]])\n",
    "    return np.array(input), np.array(label)\n",
    "\n",
    "x, y=random_batch(2,corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4724],\n",
       "       [5769]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1269],\n",
       "       [3746]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec(Skipgram)\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size   = len(vocabs)\n",
    "emb_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skipgram model\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_center=nn.Embedding(voc_size,emb_size)\n",
    "        self.embedding_outside=nn.Embedding(voc_size,emb_size)\n",
    "    def forward(self,center,outside,all_vocabs):\n",
    "        center_embedding=self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embedding=self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding=self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size\n",
    "\n",
    "        top_term=torch.exp(outside_embedding.bmm(center_embedding.transpose(1,2)).squeeze(2))  # bmm is dot product (ignore batch size) and reduce dim to 2\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        lower_term=all_vocabs_embedding.bmm(center_embedding.transpose(1,2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum=torch.sum(torch.exp(lower_term),1) #(batch_size,1)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss=-torch.mean(torch.log(top_term/lower_term_sum))\n",
    "        \n",
    "        return loss\n",
    "    def get_vector(self, word):\n",
    "        id_tensor = torch.LongTensor([word2index[word]])\n",
    "        id_tensor = id_tensor\n",
    "        v_embed = self.embedding_v(id_tensor)\n",
    "        u_embed = self.embedding_u(id_tensor) \n",
    "        word_embed = (v_embed + u_embed) / 2 \n",
    "        # x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "\n",
    "        return word_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14267, 14268, 14269],\n",
       "        [    0,     1,     2,  ..., 14267, 14268, 14269]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare all vocabs\n",
    "batch_size=2\n",
    "voc_size= len(vocabs)\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs=list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"],seq))\n",
    "    return  torch.LongTensor(idxs)\n",
    "all_vocabs=prepare_sequence(list(vocabs),word2index).expand(batch_size,voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram=Skipgram(voc_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skipgram, y_skipgram = random_batch(batch_size, corpus)\n",
    "x_tensor_skipgram = torch.LongTensor(x_skipgram)\n",
    "y_tensor_skipgram = torch.LongTensor(y_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5296, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_skipgram = model_skipgram(x_tensor_skipgram, y_tensor_skipgram, all_vocabs)\n",
    "loss_skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 2 # mini-batch size\n",
    "model_skipgram      = Skipgram(voc_size, emb_size)\n",
    "optimizer_skipgram  = optim.Adam(model_skipgram.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    mins = elapsed_time // 60\n",
    "    secs = elapsed_time % 60\n",
    "    return int(mins), int(secs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 | Loss: 10.956932\n",
      "Epoch 200 | Loss: 9.925303\n",
      "Epoch 300 | Loss: 8.831079\n",
      "Epoch 400 | Loss: 10.877717\n",
      "Epoch 500 | Loss: 9.682178\n",
      "Epoch 600 | Loss: 10.013674\n",
      "Epoch 700 | Loss: 9.777369\n",
      "Epoch 800 | Loss: 14.556937\n",
      "Epoch 900 | Loss: 10.726488\n",
      "Epoch 1000 | Loss: 9.682384\n",
      "time: 9m 42s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "num_epochs = 1000\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch_skipgram, label_batch_skipgram = random_batch(batch_size, corpus)\n",
    "    input_tensor_skipgram = torch.LongTensor(input_batch_skipgram)\n",
    "    label_tensor_skipgram = torch.LongTensor(label_batch_skipgram)\n",
    "     \n",
    "    #predict\n",
    "    loss_skipgram = model_skipgram(input_tensor_skipgram, label_tensor_skipgram, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_skipgram.zero_grad()\n",
    "    loss_skipgram.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_skipgram.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss_skipgram:2.6f}\") #Epoch 6 front space, 0 back space\n",
    "\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "print(f\"time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.682384490966797, Training Time: 9m 42s\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Loss: {loss_skipgram}, Training Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the model for testing\n",
    "torch.save(model_skipgram.state_dict(), 'D:/AIT_lecture/NLP/code\\Assignment/NLP-2025/NLP-A1/models/skipgram_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data = {\n",
    "#     'corpus': corpus,\n",
    "#     'vocab': vocabs,\n",
    "#     'word2index': word2index,\n",
    "#     'voc_size': voc_size,\n",
    "#     'embedding_size': emb_size\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(Data,open('./models/Data.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonNLP",
   "language": "python",
   "name": "pythonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
