{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')  # For tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters as rt \n",
    "rt.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "categories = rt.categories() \n",
    "\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized sentences: 8458\n",
      "['china', 'daily', 'says', 'vermin', 'eat', '7-12', 'pct', 'grain', 'stocks', 'a', 'survey', 'of', '19', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', '12', 'pct', 'of', 'china', \"'s\", 'grain', 'stocks', ',', 'the', 'china', 'daily', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "#. tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Get raw text from the reuters corpus\n",
    "raw_text = rt.raw(categories=['fuel', 'gas','gold', 'grain', 'heat', 'housing', 'income', 'interest']) \n",
    "\n",
    "# Tokenize into sentences and then words\n",
    "sentences = sent_tokenize(raw_text)  # Tokenize the raw text into sentences\n",
    "corpus = [word_tokenize(sent.lower()) for sent in sentences]  # Tokenize sentences into words\n",
    "\n",
    "# Check the number of tokenized sentences\n",
    "print(f\"Number of tokenized sentences: {len(corpus)}\")\n",
    "\n",
    "\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numeralizaition\n",
    "# find uniquee words\n",
    "flatten=lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs=list(set(flatten(corpus))) # all the words we have in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hand mapping between iteger and word\n",
    "word2index={v:idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_idx=len(vocabs)\n",
    "lst_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append UNK for unknown\n",
    "vocabs.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['<UNK>']= lst_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word={v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14270"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab size\n",
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of center word, and outsidee word\n",
    "def random_batch(batch_size,corpus):\n",
    "    skipgrams=[]\n",
    "    # loop  each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 3rd word until third last word since window size =2\n",
    "        for i in range(2, len(doc)-2):\n",
    "            #center word\n",
    "            center=word2index[doc[i]]\n",
    "            #outside words=2 words\n",
    "            outside = (word2index[doc[i-2]],word2index[doc[i-1]], \n",
    "                       word2index[doc[i+1]],word2index[doc[i+2]])\n",
    "            #print(center, outside)\n",
    "            #for each for these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                #print(each_out)\n",
    "                skipgrams.append([center,each_out])\n",
    "            # center, outeside1; center, outside2\n",
    "\n",
    "    random_index=np.random.choice(range(len(skipgrams)),batch_size,replace=False)\n",
    "    input, label=[], []\n",
    "    for index in random_index:\n",
    "        input.append([skipgrams[index][0]])\n",
    "        label.append([skipgrams[index][1]])\n",
    "    return np.array(input), np.array(label)\n",
    "\n",
    "x, y=random_batch(2,corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4953],\n",
       "       [12783]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3225],\n",
       "       [13529]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Negative Sampling\n",
    "### Unigram distribution\n",
    "\n",
    "$$P(w)=U(w)^{3/4}/Z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241109"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "\n",
    "#count the total number of words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size   = len(vocabs)\n",
    "emb_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 14267, 14268, 14269],\n",
       "        [    0,     1,     2,  ..., 14267, 14268, 14269]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare all vocabs\n",
    "batch_size=2\n",
    "voc_size= len(vocabs)\n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs=list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"],seq))\n",
    "    return  torch.LongTensor(idxs)\n",
    "all_vocabs=prepare_sequence(list(vocabs),word2index).expand(batch_size,voc_size)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# sample 5 words on corpus\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_neg, y_neg = random_batch(batch_size, corpus)\n",
    "x_tensor_neg = torch.LongTensor(x_neg)\n",
    "y_tensor_neg = torch.LongTensor(y_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "# check negative_sampling\n",
    "neg_samples = negative_sampling(y_tensor_neg, unigram_table, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5952])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tensor_neg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4204,  9695,  7608,  4435, 13810])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skipgram negative sampling\n",
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.center_embedding  = nn.Embedding(voc_size, emb_size)\n",
    "        self.outside_embedding = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.center_embedding(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.outside_embedding(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.outside_embedding(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1) #sum on second dim\n",
    "        \n",
    "        # calculate loss\n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        id_tensor = torch.LongTensor([word2index[word]])\n",
    "        id_tensor = id_tensor\n",
    "        v_embed = self.embedding_center(id_tensor)  # Corrected\n",
    "        u_embed = self.embedding_outside(id_tensor)  # Corrected\n",
    "        word_embed = (v_embed + u_embed) / 2 \n",
    "\n",
    "        return word_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#testing my model\n",
    "test_model_neg = SkipgramNeg(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7395, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_neg = test_model_neg(x_tensor_neg, y_tensor_neg, neg_samples)\n",
    "loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    mins = elapsed_time // 60\n",
    "    secs = elapsed_time % 60\n",
    "    return int(mins), int(secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neg     = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer_neg = optim.Adam(model_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    100 | Loss: 0.752042\n",
      "Epoch    200 | Loss: 1.440988\n",
      "Epoch    300 | Loss: 2.100292\n",
      "Epoch    400 | Loss: 1.283021\n",
      "Epoch    500 | Loss: 3.125569\n",
      "Epoch    600 | Loss: 3.268190\n",
      "Epoch    700 | Loss: 2.173976\n",
      "Epoch    800 | Loss: 1.126065\n",
      "Epoch    900 | Loss: 1.351720\n",
      "Epoch   1000 | Loss: 3.110745\n",
      "time: 10m 30s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import time\n",
    "num_epochs = 1000\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch_neg, label_batch_neg = random_batch(batch_size, corpus)\n",
    "    input_tensor_neg = torch.LongTensor(input_batch_neg)\n",
    "    label_tensor_neg = torch.LongTensor(label_batch_neg)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor_neg, unigram_table, k)\n",
    "    loss_neg = model_neg(input_tensor_neg, label_tensor_neg, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer_neg.zero_grad()\n",
    "    loss_neg.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer_neg.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss_neg:2.6f}\")\n",
    "\n",
    "end = time.time()\n",
    "epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "print(f\"time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.1107451915740967, Training Time: 10m 30s\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Loss: {loss_neg}, Training Time: {epoch_mins}m {epoch_secs}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for testing\n",
    "torch.save(model_neg.state_dict(), 'D:/AIT_lecture/NLP/code\\Assignment/NLP-2025/NLP-A1/models/skipgramNeg_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "Data = pickle.load(open(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\Data.pkl', 'rb'))\n",
    "\n",
    "corpus = Data['corpus']\n",
    "vocab = Data['vocab']\n",
    "word2index = Data['word2index']\n",
    "voc_size = Data['voc_size']\n",
    "embed_size = Data['embedding_size']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonNLP",
   "language": "python",
   "name": "pythonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
