{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_function import Skipgram, SkipgramNeg, Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing training data\n",
    "Data = pickle.load(open(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\Data.pkl', 'rb'))\n",
    "\n",
    "corpus = Data['corpus']\n",
    "vocab = Data['vocab']\n",
    "word2index = Data['word2index']\n",
    "voc_size = Data['voc_size']\n",
    "embed_size = Data['embedding_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4304\\217274521.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\skipgram_v1.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Skipgram(\n",
       "  (embedding_center): Embedding(14270, 2)\n",
       "  (embedding_outside): Embedding(14270, 2)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the state_dict\n",
    "state_dict = torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\skipgram_v1.pt')\n",
    "\n",
    "# Initialize the Skipgram model with the appropriate vocab_size and embed_size\n",
    "skipgram = Skipgram(voc_size, embed_size)\n",
    "\n",
    "# Load the state_dict into the model\n",
    "skipgram.load_state_dict(state_dict,strict=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "skipgram.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4304\\1708150165.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\skipgramNeg_v1.pt')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SkipgramNeg(\n",
       "  (embedding_center): Embedding(14270, 2)\n",
       "  (embedding_outside): Embedding(14270, 2)\n",
       "  (logsigmoid): LogSigmoid()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the state_dict\n",
    "state_dict = torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\skipgramNeg_v1.pt')\n",
    "\n",
    "# Remap keys\n",
    "state_dict_remapped = {\n",
    "    'embedding_center.weight': state_dict['center_embedding.weight'],\n",
    "    'embedding_outside.weight': state_dict['outside_embedding.weight'],\n",
    "}\n",
    "\n",
    "# Load the remapped state_dict into the model\n",
    "skipgramNeg = SkipgramNeg(voc_size, embed_size)\n",
    "skipgramNeg.load_state_dict(state_dict_remapped)\n",
    "skipgramNeg.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4304\\1039450169.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  glove.load_state_dict(torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\GloVe-v1.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Glove(\n",
       "  (center_embedding): Embedding(14270, 2)\n",
       "  (outside_embedding): Embedding(14270, 2)\n",
       "  (center_bias): Embedding(14270, 1)\n",
       "  (outside_bias): Embedding(14270, 1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #Initialize the model and load the pre-trained weights from the saved file\n",
    "glove = Glove(voc_size, embed_size)\n",
    "glove.load_state_dict(torch.load(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\models\\GloVe-v1.pt'))\n",
    "glove.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search glove.6G.100d.txt from google, and save \n",
    "glove_file = datapath('glove.6B.100d.txt')  \n",
    "gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_all_word_vectors(vocab, model):\n",
    "    \"\"\"\n",
    "    Compute vectors for all words in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        vocab (list): List of vocabulary words.\n",
    "        model (object): Pretrained embedding model.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing vectors for all vocabulary words.\n",
    "    \"\"\"\n",
    "    word_vectors = []\n",
    "    for word in vocab:\n",
    "        word_vectors.append(model.get_vector(word))\n",
    "    return torch.stack(word_vectors)\n",
    "\n",
    "def similarities(lines, model, vocab):\n",
    "    \"\"\"\n",
    "    Perform semantic and syntactic analysis using word vector similarities.\n",
    "    \n",
    "    Args:\n",
    "        lines (list): List of analogies (each line has four words: a, b, c, d).\n",
    "        model (object): Pretrained embedding model.\n",
    "        vocab (list): List of vocabulary words.\n",
    "    \n",
    "    Returns:\n",
    "        float: Accuracy of the analogies.\n",
    "    \"\"\"\n",
    "    # Compute all word vectors once\n",
    "    all_word_vectors = compute_all_word_vectors(vocab, model)\n",
    "\n",
    "    correct = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "\n",
    "        # Skip lines with unknown words\n",
    "        if any(word not in vocab for word in words):\n",
    "            skipped += 1\n",
    "            # print(f\"Skipping analogy due to unknown words: {line}\")\n",
    "            continue\n",
    "\n",
    "        # Retrieve vectors for analogy words\n",
    "        vectors = [model.get_vector(word.lower()) for word in words]\n",
    "\n",
    "        # Perform vector manipulation\n",
    "        result_vector = vectors[1] - vectors[0] + vectors[2]\n",
    "        result_vector = result_vector.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Calculate cosine similarities\n",
    "        similarities = F.cosine_similarity(result_vector, all_word_vectors)\n",
    "\n",
    "        # Get the closest word\n",
    "        closest_word_index = torch.argmax(similarities).item()\n",
    "        closest_word = vocab[closest_word_index]\n",
    "\n",
    "        if closest_word == words[3]:  # Check if predicted word matches target\n",
    "            correct += 1\n",
    "        # else:\n",
    "        #     print(f\"Mismatch: {line} -> Predicted: {closest_word}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total = len(lines) - skipped\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print('---------------------------------------------------------')\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Skipped: {skipped} analogies (unknown words)')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Semantic and Syntatic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file and create a list of tuples\n",
    "with open(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\dataset\\semantic_capital_country.txt', 'r') as file:\n",
    "    sem_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file and create a list of tuples\n",
    "with open(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\dataset\\syntatic_past_tense.txt', 'r') as file:\n",
    "    syn_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work2Vec(Skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 506 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(sem_lines, skipgram,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 1008 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(syn_lines, skipgram,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram Negative Sampling model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 506 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(sem_lines, skipgramNeg,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 1008 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(syn_lines, skipgramNeg,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 506 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(sem_lines, glove,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Accuracy: 0.00%\n",
      "Skipped: 1008 analogies (unknown words)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities(syn_lines, glove,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe(Gensim) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_glove(lines, model):\n",
    "    \"\"\"\n",
    "    Evaluate GloVe (Gensim) model on word analogy tasks.\n",
    "\n",
    "    Args:\n",
    "        lines (list): List of analogy lines (e.g., \"king queen man woman\").\n",
    "        model: Gensim KeyedVectors model (e.g., GloVe).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the analogy task.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.lower().split()  # Convert line to lowercase and split into words\n",
    "\n",
    "        # Check if line is valid and all words exist in the model\n",
    "        if len(words) != 4:\n",
    "            print(f\"Skipping malformed line: {line}\")\n",
    "            continue\n",
    "        if any(word not in model for word in words):\n",
    "            print(f\"Skipping line due to OOV words: {line}\")\n",
    "            continue\n",
    "\n",
    "        # Perform analogy\n",
    "        try:\n",
    "            result = model.most_similar(positive=[words[2], words[1]], negative=[words[0]], topn=1)\n",
    "            closest_word = result[0][0]  # Get the most similar word\n",
    "            total += 1\n",
    "\n",
    "            if closest_word == words[3]:\n",
    "                correct += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {line}: {e}\")\n",
    "            continue\n",
    "\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print('---------------------------------------------------------')\n",
    "    print(f'Total lines evaluated: {total}')\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Total lines evaluated: 506\n",
      "Accuracy: 93.87%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.87351778656127"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_glove(sem_lines,gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Total lines evaluated: 1560\n",
      "Accuracy: 55.45%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55.44871794871795"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_glove(syn_lines,gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file and create a list of tuples\n",
    "with open(r'D:\\AIT_lecture\\NLP\\code\\Assignment\\NLP-2025\\NLP-A1\\dataset\\wordsim_similarity_goldstandard.txt', 'r') as file:\n",
    "    similarity_lines = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity between two vectors \\( A \\) and \\( B \\) is calculated using the formula:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors A and B.\n",
    "\n",
    "    Args:\n",
    "        A (numpy array): First vector.\n",
    "        B (numpy array): Second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity between A and B.\n",
    "    \"\"\"\n",
    "    # Flatten vectors to ensure they're 1D\n",
    "    A = A.flatten()\n",
    "    B = B.flatten()\n",
    "\n",
    "    # Compute dot product and norms\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "\n",
    "    # Return cosine similarity\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def similar(lines, model):\n",
    "    \"\"\"\n",
    "    Evaluate word similarity using cosine similarity and Spearman rank correlation.\n",
    "\n",
    "    Args:\n",
    "        lines (list of str): List of sentences with 3 parts: word1, word2, real_similarity_score.\n",
    "        model (gensim model): Pretrained word embedding model (e.g., GloVe, Word2Vec).\n",
    "\n",
    "    Returns:\n",
    "        spearmanr object: Spearman rank correlation of predicted vs actual similarity scores.\n",
    "    \"\"\"\n",
    "    scores_real = []  # To store actual similarity scores (from the dataset)\n",
    "    scores_pred = []  # To store predicted similarity scores (using cosine similarity)\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()  # Split line into words\n",
    "        vec = []  # List to store word vectors\n",
    "\n",
    "        # Assuming the first two words need to be compared\n",
    "        for word in words[:2]: \n",
    "            try:\n",
    "                # Attempt to get the vector for the word\n",
    "                vec.append(model.get_vector(word).detach().numpy())\n",
    "            except:\n",
    "                # If the word is not in the vocabulary, use the <UNK> token\n",
    "                vec.append(model.get_vector('<UNK>').detach().numpy())\n",
    "\n",
    "        # Store the actual similarity score from the dataset (third word)\n",
    "        scores_real.append(float(words[2]))  \n",
    "        \n",
    "        # Calculate the cosine similarity between the two words and store the predicted score\n",
    "        scores_pred.append(cosine_similarity(np.array(vec[0]), np.array(vec[1])))\n",
    "\n",
    "    # Calculate and return Spearman's rank correlation between actual and predicted scores\n",
    "    return spearmanr(scores_real, scores_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram correlation score: 0.19211968837499357\n"
     ]
    }
   ],
   "source": [
    "print(f'Skipgram correlation score: {similar(similarity_lines,skipgram)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram (Negative sampling) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram (Neg) correlation score: 0.12842785877674645\n"
     ]
    }
   ],
   "source": [
    "print(f'Skipgram (Neg) correlation score: {similar(similarity_lines,skipgramNeg)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLove correlation score: 0.25997528190281816\n"
     ]
    }
   ],
   "source": [
    "print(f'GLove correlation score: {similar(similarity_lines,glove)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Gensim) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_gensim(lines, model):\n",
    "    scores_real = []  # Store real human similarity scores\n",
    "    scores_pred = []  # Store predicted cosine similarities based on embeddings\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()  # Split each line into words\n",
    "        vec = []\n",
    "        \n",
    "        # Extract word vectors for the first two words\n",
    "        for word in words[:2]:\n",
    "            try:\n",
    "                # Use model[word] to get the embedding directly from the model (Gensim)\n",
    "                vec.append(model[word])\n",
    "            except KeyError:\n",
    "                # Handle missing words by using a placeholder or a default embedding\n",
    "                # Here I use 'unk' as an example. Adjust based on your vocabulary.\n",
    "                vec.append(model['unk'])  # You can use your own word for unknown words.\n",
    "        \n",
    "        # Append human similarity score (the third element of each line)\n",
    "        scores_real.append(float(words[2]))\n",
    "\n",
    "        # Compute the predicted similarity using cosine similarity\n",
    "        similarity_score = cosine_similarity(np.array(vec[0]), np.array(vec[1]))\n",
    "        scores_pred.append(similarity_score)\n",
    "\n",
    "    # Calculate Spearman's rank correlation between real and predicted scores\n",
    "    correlation, p_value = spearmanr(scores_real, scores_pred)\n",
    "\n",
    "    # print(f\"Spearman Rank Correlation of Gensim: {correlation:.4f}\")\n",
    "    # print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    return correlation, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gensim correlation score: 0.6035596038474791\n"
     ]
    }
   ],
   "source": [
    "print(f'Gensim correlation score: {similar_gensim(similarity_lines,gensim)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def similar_human(lines):\n",
    "    scores_real = []  # Store human-provided similarity scores\n",
    "    scores_pred = []  # Store predicted scores (from the user)\n",
    "\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        scores_real.append(float(words[2]))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                # Ask the user for input\n",
    "                human_score = float(input(f\"How would you rate the relation between '{words[0]}' and '{words[1]}' on a scale of 0 to 10: \"))\n",
    "                \n",
    "                # Check if the input is within the valid range (0 to 10)\n",
    "                if 0 <= human_score <= 10:\n",
    "                    scores_pred.append(human_score)\n",
    "                    break  # Exit the loop if the input is valid\n",
    "                else:\n",
    "                    print(\"Invalid input. Please enter a score between 0 and 10.\")\n",
    "\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a numeric value.\")\n",
    "\n",
    "    # Compute Spearman's Rank Correlation\n",
    "    correlation, p_value = spearmanr(scores_real, scores_pred)\n",
    "\n",
    "    return correlation, p_value\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid input. Please enter a numeric value.\n",
      "Invalid input. Please enter a numeric value.\n",
      "Invalid input. Please enter a numeric value.\n",
      "Invalid input. Please enter a numeric value.\n",
      "Human correlation score: 0.8735\n"
     ]
    }
   ],
   "source": [
    "correlation, p_value = similar_human(similarity_lines)\n",
    "print(f\"Human correlation score: {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test score comparison are provided in README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonNLP",
   "language": "python",
   "name": "pythonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
