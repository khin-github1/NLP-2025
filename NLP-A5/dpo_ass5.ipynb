{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load a pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT_lecture\\NLP\\pythonNLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# Model and tokenizer\n",
    "\n",
    "\n",
    "model_name_or_path = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "if ignore_bias_buffers:\n",
    "    # torch distributed hack\n",
    "    model._ddp_params_and_buffers_to_ignore = [\n",
    "        name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "    ]\n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Finding a suitable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract prompt from response\n",
    "def extract_anthropic_prompt(prompt_and_response: str) -> str:\n",
    "    search_term = \"\\n\\nAssistant:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "# Load dataset\n",
    "def get_static_hh(split: str, sanity_check: bool = False, cache_dir: str = None):\n",
    "    dataset = load_dataset(\"Dahoas/static-hh\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 5)))  # Use a smaller dataset for testing\n",
    "\n",
    "    def filter_columns(sample):\n",
    "        return {\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"chosen\": sample[\"chosen\"],\n",
    "            \"rejected\": sample[\"rejected\"],\n",
    "        }\n",
    "\n",
    "    return dataset.map(filter_columns)\n",
    "\n",
    "# Prepare datasets\n",
    "sanity_check = True  # Set to False for full dataset\n",
    "train_dataset = get_static_hh(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_static_hh(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Task 2. Training a Model with DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with lr=5e-05, batch_size=4, epochs=3, beta=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT_lecture\\NLP\\pythonNLP\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24132\\1273761191.py:36: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 04:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.033892</td>\n",
       "      <td>-1.894423</td>\n",
       "      <td>-1.620894</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.273529</td>\n",
       "      <td>-284.840820</td>\n",
       "      <td>-126.990891</td>\n",
       "      <td>-3.056025</td>\n",
       "      <td>-3.120090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.445939</td>\n",
       "      <td>-3.567740</td>\n",
       "      <td>-2.634429</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.933312</td>\n",
       "      <td>-301.573975</td>\n",
       "      <td>-137.126236</td>\n",
       "      <td>-3.084869</td>\n",
       "      <td>-3.164944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.634201</td>\n",
       "      <td>-4.152308</td>\n",
       "      <td>-2.937025</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-1.215283</td>\n",
       "      <td>-307.419647</td>\n",
       "      <td>-140.152206</td>\n",
       "      <td>-3.097786</td>\n",
       "      <td>-3.176896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model found! Saving model at: ./dpo_lr5e-05_bs4_ep3_beta0.1\n",
      "\n",
      "Experiment Results:\n",
      "{'learning_rate': 5e-05, 'batch_size': 4, 'epochs': 3, 'beta': 0.1, 'loss': 1.6342010498046875}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter\n",
    "learning_rates = [5e-5]\n",
    "batch_sizes = [4]\n",
    "num_epochs = [3]\n",
    "betas = [0.1]\n",
    "\n",
    "# Generate all possible hyperparameter combinations\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, batch_sizes, num_epochs, betas))\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_loss = float(\"inf\")  # Initialize best loss as infinity\n",
    "best_model_path = None\n",
    "\n",
    "for lr, batch_size, epochs, beta in hyperparameter_combinations:\n",
    "    print(f\"\\nTraining with lr={lr}, batch_size={batch_size}, epochs={epochs}, beta={beta}\")\n",
    "    output_dir = f\"./dpo_lr{lr}_bs{batch_size}_ep{epochs}_beta{beta}\"\n",
    "\n",
    "    # DPO training configuration\n",
    "    dpo_config = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=lr,\n",
    "        report_to=\"none\",\n",
    "        beta=beta,  # Temperature parameter for preference weighting\n",
    "    )\n",
    "\n",
    "    # Initialize DPOTrainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model,\n",
    "        args=dpo_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    dpo_trainer.train()\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_results = dpo_trainer.evaluate()\n",
    "    loss = eval_results.get(\"eval_loss\", None)\n",
    "    results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"beta\": beta,\n",
    "        \"loss\": loss\n",
    "    })\n",
    "    if loss is not None and loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_path = output_dir\n",
    "        print(f\"New best model found! Saving model at: {best_model_path}\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nExperiment Results:\")\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AIT_lecture\\NLP\\pythonNLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the path to the best model (replace with your actual best_model_path)\n",
    "repo_id = \"khinhlaing/dop_qwan\"  # model from hugging face repo\n",
    "\n",
    "# Load the fine-tuned model\n",
    "best_model = AutoModelForCausalLM.from_pretrained(repo_id)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "\n",
    "# Ensure padding token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: How to study effectively?\n",
      "Response: 1. Set up a study schedule, make sure you have all the resources and tools needed.\n",
      "2. Take notes on everything you read, practice good reading habits, and make sure you understand each chapter.\n",
      "3. Join a club or organization that interests you, share your knowledge with others and get help from experienced mentors.\n",
      "4. Practice active listening, be prepared to ask questions and give feedback, and use active learning techniques such as summarizing, making connections and using technology like Google Docs.\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Define a single sample prompt\n",
    "sample_prompt = \"How to study effectively?\"\n",
    "\n",
    "# Function to generate a response without repeating the prompt\n",
    "def generate_response(prompt, max_tokens=100):\n",
    "    try:\n",
    "        # Format input as a dialogue\n",
    "        formatted_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "\n",
    "        # Tokenize the input\n",
    "        input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # Generate a response\n",
    "        with torch.no_grad():\n",
    "            output_ids = best_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,  # Controls output length\n",
    "                temperature=0.7,  # Adds diversity\n",
    "                top_p=0.9,  # Nucleus sampling\n",
    "                do_sample=True,  # Enables varied responses\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Handles padding properly\n",
    "            )\n",
    "\n",
    "        # Decode and clean response\n",
    "        full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = full_response.replace(formatted_prompt, \"\").strip()\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(sample_prompt)\n",
    "print(f\"Prompt: {sample_prompt}\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonNLP",
   "language": "python",
   "name": "pythonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
